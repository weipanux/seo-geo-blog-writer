#!/usr/bin/env python3
"""
Competitor Content Analysis for SEO-GEO Blog Writer

Powered by DataForSEO APIs for professional-grade analysis:
1. Get top-ranking pages for your keyword (SERP API)
2. Analyze each page with OnPage API (word count, readability, SEO scores, etc.)
3. Return structured competitive intelligence for Claude to use

Usage (by Claude):
    python competitor_analysis.py "best wireless earbuds 2025"
    python competitor_analysis.py "healthy boundaries" --limit 5 --location US

Manual testing:
    python competitor_analysis.py "topic" --format json
    python competitor_analysis.py "topic" --output analysis.md

Cost: ~$0.05 per keyword
  - SERP API: $0.05 (get URLs)
  - OnPage API: $0.00125 (10 pages √ó $0.000125)
  - Total: ~$0.05125

Features vs homebrew scraping:
  ‚úÖ 5 readability metrics (Flesch-Kincaid, etc.)
  ‚úÖ SEO health scores (0-100)
  ‚úÖ Content consistency metrics
  ‚úÖ Social media tag analysis
  ‚úÖ 50+ automated quality checks
  ‚úÖ 100% reliable (no scraping failures)
"""

import argparse
import json
import re
import sys
from pathlib import Path
from typing import Dict, List, Optional
from dataclasses import dataclass, asdict
from urllib.parse import urlparse

try:
    import requests
    from requests.auth import HTTPBasicAuth
except ImportError:
    print("Error: 'requests' library required. Install: pip install -r requirements.txt", file=sys.stderr)
    sys.exit(1)

# Import credential management from keyword_research
sys.path.insert(0, str(Path(__file__).parent))
try:
    from keyword_research import get_api_key
except ImportError:
    print("Error: keyword_research.py not found. Ensure it's in the same directory.", file=sys.stderr)
    sys.exit(1)


@dataclass
class PageStructure:
    """Analyzed structure of a competitor page (from DataForSEO OnPage API)"""
    url: str
    title: str
    meta_description: str
    word_count: int
    headings: Dict[str, List[str]]  # {H1: [...], H2: [...], H3: [...]}
    schema_types: List[str]  # Inferred from social tags and micromarkup
    internal_links: int
    external_links: int
    images: int
    # Rich metrics from OnPage API
    onpage_score: float  # SEO health score (0-100)
    readability_score: float  # Flesch-Kincaid readability
    content_consistency: float  # Title-to-content consistency
    has_og_tags: bool  # Open Graph optimization
    has_twitter_tags: bool  # Twitter Card optimization

    def to_dict(self) -> Dict:
        return asdict(self)


@dataclass
class CompetitorAnalysis:
    """Complete competitor analysis for a keyword"""
    keyword: str
    location: str
    analyzed_pages: List[PageStructure]
    average_word_count: int
    target_word_count: int  # Recommended (20% longer than average)
    average_readability: float  # Average Flesch-Kincaid score
    average_seo_score: float  # Average OnPage score
    common_headings: List[str]
    common_schema: List[str]
    writing_guidelines: Dict[str, str]  # Structured guidelines for Claude

    def to_dict(self) -> Dict:
        return {
            **asdict(self),
            'analyzed_pages': [p.to_dict() for p in self.analyzed_pages]
        }


class CompetitorAnalyzer:
    """Analyze top-ranking competitor pages for a keyword"""

    def __init__(self, api_key: Optional[str] = None, interactive: bool = False):
        """
        Initialize competitor analyzer.

        Args:
            api_key: DataForSEO API key (optional, uses credential management)
            interactive: Prompt for credentials if not found
        """
        self.api_key = get_api_key(api_key, interactive=interactive)
        if not self.api_key:
            print("=" * 60, file=sys.stderr)
            print("ERROR: DataForSEO API key required", file=sys.stderr)
            print("=" * 60, file=sys.stderr)
            print("\nThis tool requires the DataForSEO SERP API.", file=sys.stderr)
            print("\nCredential options:", file=sys.stderr)
            print("  1. Set DATAFORSEO_API_KEY environment variable", file=sys.stderr)
            print("  2. Create ~/.dataforseo-skill/config.json", file=sys.stderr)
            print("  3. Use --api-key 'login:password' argument", file=sys.stderr)
            print("  4. Use --interactive flag to prompt", file=sys.stderr)
            print("=" * 60 + "\n", file=sys.stderr)
            sys.exit(1)

    def analyze(self, keyword: str, location: str = "United States", limit: int = 10) -> CompetitorAnalysis:
        """
        Analyze top-ranking pages for a keyword.

        Args:
            keyword: Target keyword to analyze
            location: Geographic location for search results
            limit: Number of top pages to analyze (default: 10)

        Returns:
            CompetitorAnalysis with structured data and Claude prompt
        """
        print(f"üîç Analyzing top {limit} pages for: '{keyword}'", file=sys.stderr)
        print(f"üìç Location: {location}\n", file=sys.stderr)

        # Step 1: Get top-ranking URLs from SERP API
        print("1Ô∏è‚É£  Fetching top-ranking URLs from Google...", file=sys.stderr)
        serp_results = self._get_serp_results(keyword, location, limit)

        if not serp_results:
            print("‚ùå No SERP results found", file=sys.stderr)
            sys.exit(1)

        print(f"   ‚úì Found {len(serp_results)} ranking pages\n", file=sys.stderr)

        # Step 2: Scrape and analyze each page
        print("2Ô∏è‚É£  Scraping and analyzing page structures...", file=sys.stderr)
        analyzed_pages = []

        for i, result in enumerate(serp_results, 1):
            url = result.get('url')
            title = result.get('title', 'N/A')

            print(f"   [{i}/{len(serp_results)}] {urlparse(url).netloc}", file=sys.stderr)

            try:
                page_structure = self._analyze_page(url, title, result.get('description', ''))
                analyzed_pages.append(page_structure)
            except Exception as e:
                print(f"       ‚ö†Ô∏è  Skipped ({type(e).__name__})", file=sys.stderr)
                continue

        print(f"\n   ‚úì Successfully analyzed {len(analyzed_pages)}/{len(serp_results)} pages\n", file=sys.stderr)

        # Step 3: Aggregate insights
        print("3Ô∏è‚É£  Generating competitive insights...", file=sys.stderr)

        avg_word_count = self._calculate_average_word_count(analyzed_pages)
        avg_readability = self._calculate_average_readability(analyzed_pages)
        avg_seo_score = self._calculate_average_seo_score(analyzed_pages)
        common_headings = self._extract_common_headings(analyzed_pages)
        common_schema = self._extract_common_schema(analyzed_pages)

        print(f"   ‚úì Average word count: {avg_word_count:,}", file=sys.stderr)
        print(f"   ‚úì Average readability: {avg_readability:.1f} (Flesch-Kincaid)", file=sys.stderr)
        print(f"   ‚úì Average SEO score: {avg_seo_score:.1f}/100", file=sys.stderr)
        print(f"   ‚úì Common heading patterns: {len(common_headings)}", file=sys.stderr)
        print(f"   ‚úì Schema types found: {len(common_schema)}\n", file=sys.stderr)

        # Step 4: Build writing guidelines
        print("4Ô∏è‚É£  Building writing guidelines...", file=sys.stderr)
        target_word_count = int(avg_word_count * 1.2)  # 20% longer
        writing_guidelines = self._build_writing_guidelines(
            keyword, analyzed_pages, avg_word_count, avg_readability, avg_seo_score, common_headings, common_schema
        )
        print("   ‚úì Guidelines ready\n", file=sys.stderr)

        return CompetitorAnalysis(
            keyword=keyword,
            location=location,
            analyzed_pages=analyzed_pages,
            average_word_count=avg_word_count,
            target_word_count=target_word_count,
            average_readability=avg_readability,
            average_seo_score=avg_seo_score,
            common_headings=common_headings,
            common_schema=common_schema,
            writing_guidelines=writing_guidelines
        )

    def _get_serp_results(self, keyword: str, location: str, limit: int) -> List[Dict]:
        """
        Get top-ranking URLs from DataForSEO SERP API.

        Endpoint: POST /v3/serp/google/organic/live/advanced
        Documentation: https://docs.dataforseo.com/v3/serp/google/organic/live/advanced/
        Cost: ~$0.05 per request
        """
        try:
            # DataForSEO SERP API endpoint (live/advanced for immediate results)
            url = "https://api.dataforseo.com/v3/serp/google/organic/live/advanced"

            # Parse credentials
            login, password = self.api_key.split(':')

            # Prepare request payload
            payload = [{
                "keyword": keyword,
                "location_name": location,
                "language_code": "en",
                "device": "desktop",
                "os": "windows",
                "depth": limit  # Number of results to return
            }]

            response = requests.post(
                url,
                json=payload,
                auth=HTTPBasicAuth(login, password),
                timeout=30,
                headers={"Content-Type": "application/json"}
            )

            response.raise_for_status()
            data = response.json()

            # Parse response
            tasks = data.get('tasks', [])
            if not tasks:
                return []

            task = tasks[0]
            if task.get('status_code') != 20000:
                print(f"Warning: SERP API returned status {task.get('status_code')}", file=sys.stderr)
                return []

            result = task.get('result', [])
            if not result:
                return []

            # Extract organic results
            items = result[0].get('items', [])

            # Filter only organic results (exclude ads, featured snippets, etc.)
            organic_results = []
            for item in items:
                if item.get('type') == 'organic':
                    organic_results.append({
                        'url': item.get('url'),
                        'title': item.get('title'),
                        'description': item.get('description'),
                        'position': item.get('rank_absolute')
                    })

            return organic_results[:limit]

        except requests.exceptions.RequestException as e:
            print(f"Error calling SERP API: {e}", file=sys.stderr)
            return []
        except Exception as e:
            print(f"Error parsing SERP results: {e}", file=sys.stderr)
            return []

    def _analyze_page(self, url: str, title: str, meta_desc: str) -> PageStructure:
        """
        Analyze a single page using DataForSEO OnPage API.

        Args:
            url: Page URL to analyze
            title: Page title from SERP (fallback)
            meta_desc: Meta description from SERP (fallback)

        Returns:
            PageStructure with rich metrics from OnPage API
        """
        try:
            # DataForSEO OnPage Instant Pages endpoint
            api_url = "https://api.dataforseo.com/v3/on_page/instant_pages"

            # Parse credentials
            login, password = self.api_key.split(':')

            # Prepare request payload
            payload = [{
                "url": url,
                "enable_javascript": False,  # Faster, cheaper
                "load_resources": False,      # Don't need CSS/images
                "enable_browser_rendering": False  # Static analysis only
            }]

            response = requests.post(
                api_url,
                json=payload,
                auth=HTTPBasicAuth(login, password),
                timeout=30,
                headers={"Content-Type": "application/json"}
            )

            response.raise_for_status()
            data = response.json()

            # Parse OnPage API response
            tasks = data.get('tasks', [])
            if not tasks or tasks[0].get('status_code') != 20000:
                raise Exception(f"OnPage API error: {tasks[0].get('status_message') if tasks else 'No tasks'}")

            result = tasks[0].get('result', [])
            if not result or not result[0].get('items'):
                raise Exception("No page data in OnPage API response")

            # Extract page data
            item = result[0]['items'][0]
            meta = item.get('meta', {})
            content = meta.get('content', {})
            social_tags = meta.get('social_media_tags', {})

            # Extract headings (structured from OnPage API)
            htags = meta.get('htags', {})
            headings = {
                'H1': htags.get('h1', []),
                'H2': htags.get('h2', []),
                'H3': htags.get('h3', [])
            }

            # Infer schema types from social tags and checks
            schema_types = []
            if social_tags.get('og:type'):
                schema_types.append(social_tags['og:type'])
            if item.get('checks', {}).get('has_micromarkup'):
                schema_types.append('Micromarkup')

            # Extract metrics
            word_count = content.get('plain_text_word_count', 0)
            onpage_score = item.get('onpage_score', 0)
            readability = content.get('flesch_kincaid_readability_index', 0)
            consistency = content.get('title_to_content_consistency', 0)

            # Social media optimization
            has_og = bool(social_tags.get('og:title'))
            has_twitter = bool(social_tags.get('twitter:card'))

            return PageStructure(
                url=url,
                title=meta.get('title', title),
                meta_description=meta.get('description', meta_desc),
                word_count=word_count,
                headings=headings,
                schema_types=schema_types,
                internal_links=meta.get('internal_links_count', 0),
                external_links=meta.get('external_links_count', 0),
                images=meta.get('images_count', 0),
                onpage_score=onpage_score,
                readability_score=readability,
                content_consistency=consistency,
                has_og_tags=has_og,
                has_twitter_tags=has_twitter
            )

        except requests.exceptions.RequestException as e:
            raise Exception(f"OnPage API request failed: {e}")
        except Exception as e:
            raise Exception(f"Failed to analyze page: {e}")

    def _calculate_average_word_count(self, pages: List[PageStructure]) -> int:
        """Calculate average word count across analyzed pages"""
        if not pages:
            return 0
        return int(sum(p.word_count for p in pages) / len(pages))

    def _calculate_average_readability(self, pages: List[PageStructure]) -> float:
        """Calculate average Flesch-Kincaid readability score"""
        if not pages:
            return 0.0
        return sum(p.readability_score for p in pages) / len(pages)

    def _calculate_average_seo_score(self, pages: List[PageStructure]) -> float:
        """Calculate average OnPage SEO score"""
        if not pages:
            return 0.0
        return sum(p.onpage_score for p in pages) / len(pages)

    def _extract_common_headings(self, pages: List[PageStructure]) -> List[str]:
        """Extract common heading patterns across pages"""
        all_h2s = []
        for page in pages:
            all_h2s.extend(page.headings.get('H2', []))

        # Find patterns (simple keyword extraction)
        # This is basic - could be enhanced with NLP
        heading_keywords = {}
        for h2 in all_h2s:
            # Extract key words (3+ chars)
            words = [w.lower() for w in re.findall(r'\b\w{3,}\b', h2)]
            for word in words:
                heading_keywords[word] = heading_keywords.get(word, 0) + 1

        # Return top 10 common keywords
        sorted_keywords = sorted(heading_keywords.items(), key=lambda x: x[1], reverse=True)
        return [k for k, v in sorted_keywords[:10] if v > 1]  # Appear in 2+ headings

    def _extract_common_schema(self, pages: List[PageStructure]) -> List[str]:
        """Extract common schema types across pages"""
        schema_counts = {}
        for page in pages:
            for schema_type in page.schema_types:
                schema_counts[schema_type] = schema_counts.get(schema_type, 0) + 1

        # Return schemas that appear in 2+ pages
        return [s for s, count in schema_counts.items() if count >= 2]

    def _build_writing_guidelines(
        self,
        keyword: str,
        pages: List[PageStructure],
        avg_word_count: int,
        avg_readability: float,
        avg_seo_score: float,
        common_headings: List[str],
        common_schema: List[str]
    ) -> Dict[str, str]:
        """Build structured writing guidelines from competitor analysis"""

        # Build top competitor summary
        top_competitors = []
        for i, page in enumerate(pages[:5], 1):
            top_competitors.append({
                'rank': i,
                'domain': urlparse(page.url).netloc,
                'title': page.title,
                'h1': ', '.join(page.headings.get('H1', ['N/A'])),
                'word_count': page.word_count,
                'readability': page.readability_score,
                'seo_score': page.onpage_score,
                'top_h2s': page.headings.get('H2', [])[:3]
            })

        return {
            'target_keyword': keyword,
            'target_word_count': int(avg_word_count * 1.2),
            'target_readability': f"{avg_readability:.1f} (Flesch-Kincaid)",
            'target_seo_score': f"{avg_seo_score:.1f}/100",
            'average_competitor_word_count': avg_word_count,
            'common_topics': ', '.join(common_headings[:5]) if common_headings else 'Various',
            'schema_recommendations': ', '.join(common_schema) if common_schema else 'Article, FAQPage',
            'primary_schema': common_schema[0] if common_schema else 'Article',
            'top_5_competitors': str(top_competitors),
            'seo_focus': f"Use '{keyword}' in H1, introduction, and naturally throughout",
            'content_strategy': 'Go deeper than competitors with unique insights, data, and examples',
            'quality_targets': f"Match or exceed {avg_readability:.1f} readability and {avg_seo_score:.1f} SEO score"
        }


def format_output(analysis: CompetitorAnalysis, format_type: str = "markdown") -> str:
    """Format competitor analysis output"""

    if format_type == "json":
        return json.dumps(analysis.to_dict(), indent=2)

    elif format_type == "markdown":
        lines = [
            f"# Competitor Analysis: {analysis.keyword}",
            f"\n**Location**: {analysis.location}",
            f"**Pages Analyzed**: {len(analysis.analyzed_pages)}\n",

            "## üìä Quality Benchmarks\n",
            f"- **Average Word Count**: {analysis.average_word_count:,}",
            f"- **Target Word Count**: {analysis.target_word_count:,} (20% longer to outrank)",
            f"- **Average Readability**: {analysis.average_readability:.1f} (Flesch-Kincaid)",
            f"- **Average SEO Score**: {analysis.average_seo_score:.1f}/100",
            f"- **Common Topics**: {', '.join(analysis.common_headings[:5]) if analysis.common_headings else 'N/A'}",
            f"- **Schema Types**: {', '.join(analysis.common_schema) if analysis.common_schema else 'None found'}\n",

            "## üéØ Writing Guidelines\n"
        ]

        # Add writing guidelines
        guidelines = analysis.writing_guidelines
        lines.extend([
            f"- **Target Keyword**: {guidelines['target_keyword']}",
            f"- **Target Word Count**: {guidelines['target_word_count']:,} words",
            f"- **Target Readability**: {guidelines['target_readability']}",
            f"- **Target SEO Score**: {guidelines['target_seo_score']}",
            f"- **Common Topics to Cover**: {guidelines['common_topics']}",
            f"- **Schema Markup**: {guidelines['schema_recommendations']}",
            f"- **SEO Focus**: {guidelines['seo_focus']}",
            f"- **Content Strategy**: {guidelines['content_strategy']}",
            f"- **Quality Targets**: {guidelines['quality_targets']}\n",

            "## üèÜ Top Ranking Pages\n"
        ])

        for i, page in enumerate(analysis.analyzed_pages[:10], 1):
            lines.extend([
                f"### {i}. {urlparse(page.url).netloc}",
                f"- **URL**: {page.url}",
                f"- **Title**: {page.title}",
                f"- **Word Count**: {page.word_count:,}",
                f"- **Readability**: {page.readability_score:.1f} (Flesch-Kincaid)",
                f"- **SEO Score**: {page.onpage_score:.1f}/100",
                f"- **H1**: {', '.join(page.headings.get('H1', ['N/A']))}",
                f"- **Top H2s**: {', '.join(page.headings.get('H2', [])[:3])}",
                f"- **Schema**: {', '.join(page.schema_types) if page.schema_types else 'None'}",
                f"- **Social**: {'‚úÖ OG' if page.has_og_tags else '‚ùå OG'} | {'‚úÖ Twitter' if page.has_twitter_tags else '‚ùå Twitter'}",
                ""
            ])

        return "\n".join(lines)

    else:
        raise ValueError(f"Unknown format: {format_type}")


def main():
    """CLI interface for competitor analysis"""
    parser = argparse.ArgumentParser(
        description="Analyze top-ranking pages for a keyword to write better content"
    )
    parser.add_argument(
        "keyword",
        help="Target keyword to analyze (e.g., 'best wireless earbuds 2025')"
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=10,
        help="Number of top pages to analyze (default: 10)"
    )
    parser.add_argument(
        "--location",
        default="United States",
        help="Geographic location for search results (default: United States)"
    )
    parser.add_argument(
        "--api-key",
        help="DataForSEO API key (format: 'login:password'). Can also use DATAFORSEO_API_KEY env var"
    )
    parser.add_argument(
        "--interactive",
        action="store_true",
        help="Prompt for API key if not found"
    )
    parser.add_argument(
        "--format",
        choices=["json", "markdown"],
        default="markdown",
        help="Output format (default: markdown)"
    )
    parser.add_argument(
        "--output",
        help="Save output to file (optional)"
    )

    args = parser.parse_args()

    # Initialize analyzer
    analyzer = CompetitorAnalyzer(api_key=args.api_key, interactive=args.interactive)

    # Run analysis
    try:
        analysis = analyzer.analyze(args.keyword, args.location, args.limit)

        # Format output
        output = format_output(analysis, args.format)

        # Display or save
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(output)
            print(f"\n‚úÖ Analysis saved to: {args.output}", file=sys.stderr)
        else:
            print(output)

    except Exception as e:
        print(f"\n‚ùå Error: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
